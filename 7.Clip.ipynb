{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22409d88",
   "metadata": {},
   "source": [
    "## CLIP\n",
    "CLIP (Contrastive Language-Image Pre-Training) is a neural network trained on a variety of (image, text) pairs. It can be instructed in natural language to predict the most relevant text snippet, given an image, without directly optimizing for the task, similarly to the zero-shot capabilities of GPT-2 and 3. We found CLIP matches the performance of the original ResNet50 on ImageNet “zero-shot” without using any of the original 1.28M labeled examples, overcoming several major challenges in computer vision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ea8459d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "# Add modules to path\n",
    "sys.path.append('modules')\n",
    "\n",
    "# Import our custom utilities\n",
    "from data_utils import load_processed_data_v2, create_data_loaders\n",
    "from model_utils import CNNBaseline, train_model, evaluate_model, EfficientNetBaseline\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "203b8f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "377cabcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN101',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'ViT-B/32',\n",
       " 'ViT-B/16',\n",
       " 'ViT-L/14',\n",
       " 'ViT-L/14@336px']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## See available models\n",
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "60c230be",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
    "clip_model = clip_model.float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da23941a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    Resize(size=224, interpolation=bicubic, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    <function _convert_image_to_rgb at 0x0000017535C4C900>\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aedc08c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, image_dirs, preprocess,label_encoder, device='cuda'):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.image_dirs = image_dirs\n",
    "        self.preprocess = preprocess\n",
    "        self.device = device\n",
    "        self.label_encoder = label_encoder\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_id = row['img_id']\n",
    "\n",
    "        # Load image from any of the given directories\n",
    "        img = None\n",
    "        for part, dir_path in self.image_dirs.items():\n",
    "            full_path = os.path.join('archive', dir_path, img_id)\n",
    "            if os.path.exists(full_path):\n",
    "                try:\n",
    "                    img = Image.open(full_path).convert(\"RGB\")\n",
    "                    break\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "        # Fallback: create black image if missing\n",
    "        if img is None:\n",
    "            img = Image.new(\"RGB\", (224, 224), color=\"black\")\n",
    "\n",
    "        # Preprocess for CLIP\n",
    "        image_tensor = self.preprocess(img)\n",
    "\n",
    "        # Text input (raw string)\n",
    "        text_input = row['text_description']\n",
    "\n",
    "        # Target label → encode to int, then tensor\n",
    "        label_str = row['diagnostic']\n",
    "        label_idx = self.label_encoder.transform([label_str])[0]  # integer class\n",
    "        target = torch.tensor(label_idx, dtype=torch.long)\n",
    "\n",
    "        return image_tensor, text_input, target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03efdd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df, test_df, label_encoder, config= load_processed_data_v2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9dcdb4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_DIRS = {\n",
    "    'part1': 'imgs_part_1/imgs_part_1',\n",
    "    'part2': 'imgs_part_2/imgs_part_2', \n",
    "    'part3': 'imgs_part_3/imgs_part_3'\n",
    "}\n",
    "\n",
    "train_dataset = CLIPDataset(train_df, IMAGE_DIRS, preprocess,label_encoder, device=device) \n",
    "val_dataset = CLIPDataset(val_df, IMAGE_DIRS, preprocess,label_encoder, device=device) \n",
    "test_dataset = CLIPDataset(test_df, IMAGE_DIRS, preprocess,label_encoder, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "145fdd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a279827c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n",
      "('66-year-old male lesion on neck risk factors: smoke, pesticide', '53-year-old lesion on chest', '73-year-old female lesion on face risk factors: skin cancer history', '62-year-old female lesion on nose risk factors: drink, skin cancer history, cancer history', '39-year-old lesion on face', '71-year-old female lesion on chest risk factors: cancer history', '91-year-old female lesion on face', '69-year-old male lesion on forearm risk factors: cancer history', '85-year-old female lesion on ear risk factors: pesticide', '78-year-old female lesion on face risk factors: skin cancer history', '55-year-old male lesion on forearm risk factors: drink, pesticide, skin cancer history, cancer history', '64-year-old male lesion on face risk factors: drink, skin cancer history, cancer history', '77-year-old female lesion on face', '55-year-old male lesion on neck risk factors: drink, pesticide, skin cancer history', '54-year-old lesion on forearm', '73-year-old lesion on forearm', '55-year-old female lesion on chest risk factors: skin cancer history, cancer history', '41-year-old lesion on chest', '61-year-old female lesion on nose risk factors: pesticide, skin cancer history, cancer history', '62-year-old female lesion on face risk factors: smoke, skin cancer history, cancer history', '80-year-old male lesion on hand risk factors: drink, pesticide, cancer history', '64-year-old female lesion on forearm risk factors: skin cancer history, cancer history', '52-year-old lesion on face', '72-year-old lesion on face', '71-year-old female lesion on face risk factors: cancer history', '39-year-old male lesion on face risk factors: drink, pesticide, cancer history', '80-year-old female lesion on forearm risk factors: skin cancer history', '45-year-old lesion on forearm', '49-year-old female lesion on chest risk factors: skin cancer history, cancer history', '47-year-old male lesion on back risk factors: drink, pesticide', '51-year-old female lesion on forearm risk factors: skin cancer history', '78-year-old female lesion on face risk factors: pesticide, skin cancer history, cancer history')\n",
      "tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,\n",
      "        1, 1, 0, 0, 1, 1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "for images, texts, targets in train_loader:\n",
    "    print(images.shape)  # Should be [batch_size, 3, 224, 224]\n",
    "    print(texts)        # List of text descriptions\n",
    "    print(targets)      # Tensor of target labels\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8f01f037",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, in_dim, proj_dim=512, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, proj_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(proj_dim, proj_dim),\n",
    "            nn.LayerNorm(proj_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class GatedFusion(nn.Module):\n",
    "    \"\"\"Learnable gate between two modality vectors (x and y)\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(dim * 2, dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x, y):\n",
    "        # x,y shape: (B, dim)\n",
    "        z = torch.cat([x, y], dim=1)\n",
    "        g = self.gate(z)  # (B, dim)\n",
    "        return g * x + (1 - g) * y\n",
    "\n",
    "class MultimodalClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        clip_model,\n",
    "        image_dim=512,\n",
    "        text_dim=512,\n",
    "        proj_dim=512,\n",
    "        fusion_method=\"concat\",   # \"concat\", \"mean\", \"gated\"\n",
    "        num_classes=2,\n",
    "        dropout=0.3,\n",
    "        freeze_clip=True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        clip_model: loaded clip model (OpenAI), used optionally for fine-tuning or just to encode\n",
    "        image_dim, text_dim: output dims from CLIP encoders (usually 512)\n",
    "        proj_dim: projection dimension (if using projection heads)\n",
    "        fusion_method: 'concat'|'mean'|'gated'\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.clip = clip_model\n",
    "        self.freeze_clip = freeze_clip\n",
    "\n",
    "        # Optionally freeze CLIP parameters\n",
    "        if freeze_clip:\n",
    "            for p in self.clip.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        # Projection heads (bring both to proj_dim)\n",
    "        self.image_proj = ProjectionHead(image_dim, proj_dim=proj_dim)\n",
    "        self.text_proj = ProjectionHead(text_dim, proj_dim=proj_dim)\n",
    "\n",
    "        self.fusion_method = fusion_method\n",
    "        if fusion_method == \"gated\":\n",
    "            self.fuser = GatedFusion(proj_dim)\n",
    "\n",
    "        # Classifier input dim depends on fusion method\n",
    "        if fusion_method == \"concat\":\n",
    "            clf_in = proj_dim * 2\n",
    "        elif fusion_method in (\"mean\", \"gated\"):\n",
    "            clf_in = proj_dim\n",
    "        else:\n",
    "            raise ValueError(\"Unknown fusion method\")\n",
    "\n",
    "        # Classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(clf_in, clf_in // 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(clf_in // 2, num_classes)\n",
    "        )\n",
    "\n",
    "    def encode_image(self, images):\n",
    "        # images: Tensor [B, 3, H, W]\n",
    "        return self.clip.encode_image(images)\n",
    "\n",
    "    def encode_text(self, tokenized_text):\n",
    "        # tokenized_text: Tensor [B, 77]\n",
    "        return self.clip.encode_text(tokenized_text)\n",
    "\n",
    "    def forward(self, images, tokenized_text):\n",
    "        \"\"\"\n",
    "        Forward pass: encode image & text via CLIP, fuse their embeddings, classify.\n",
    "        \"\"\"\n",
    "        # Encode using CLIP\n",
    "        image_feats = self.encode_image(images)        # (B, image_dim)\n",
    "        text_feats  = self.encode_text(tokenized_text) # (B, text_dim)\n",
    "\n",
    "        # Normalize embeddings (stabilizes training)\n",
    "        image_feats = image_feats / image_feats.norm(dim=1, keepdim=True).clamp(min=1e-6)\n",
    "        text_feats  = text_feats / text_feats.norm(dim=1, keepdim=True).clamp(min=1e-6)\n",
    "\n",
    "        # Ensure consistent dtype between CLIP outputs and our projection/classifier\n",
    "        # (prevents errors like: \"mat1 and mat2 must have the same dtype, but got Half and Float\")\n",
    "        image_feats = image_feats.float()\n",
    "        text_feats = text_feats.float()\n",
    "\n",
    "        # Project both to a common dimension\n",
    "        img_p = self.image_proj(image_feats)\n",
    "        txt_p = self.text_proj(text_feats)\n",
    "\n",
    "        # Fuse (simple concatenation)\n",
    "        fused = torch.cat([img_p, txt_p], dim=1)\n",
    "\n",
    "        # Classify\n",
    "        logits = self.classifier(fused)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f608cb1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(label_encoder.classes_) \n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "198c6107",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                              lr=1e-4, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.5, patience=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f0532b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultimodalClassifier(\n",
    "    clip_model=clip_model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a7098f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00 | Train Loss: 0.7535, Acc: 0.4624, F1: 0.3040 | Val Loss: 0.7516, Acc: 0.4635, F1: 0.2936 | Time: 69.8s\n",
      "[NOTE]Saved best model.\n",
      "[NOTE]Saved best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01 | Train Loss: 0.7618, Acc: 0.4603, F1: 0.3013 | Val Loss: 0.7516, Acc: 0.4635, F1: 0.2936 | Time: 67.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 02 | Train Loss: 0.7535, Acc: 0.4624, F1: 0.3022 | Val Loss: 0.7516, Acc: 0.4635, F1: 0.2936 | Time: 69.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 03 | Train Loss: 0.7560, Acc: 0.4624, F1: 0.3004 | Val Loss: 0.7516, Acc: 0.4635, F1: 0.2936 | Time: 76.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 04 | Train Loss: 0.7601, Acc: 0.4561, F1: 0.2958 | Val Loss: 0.7516, Acc: 0.4635, F1: 0.2936 | Time: 75.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 05 | Train Loss: 0.7567, Acc: 0.4550, F1: 0.2988 | Val Loss: 0.7516, Acc: 0.4635, F1: 0.2936 | Time: 76.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 06 | Train Loss: 0.7545, Acc: 0.4656, F1: 0.3037 | Val Loss: 0.7516, Acc: 0.4635, F1: 0.2936 | Time: 78.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 07 | Train Loss: 0.7571, Acc: 0.4614, F1: 0.3000 | Val Loss: 0.7516, Acc: 0.4635, F1: 0.2936 | Time: 80.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 08 | Train Loss: 0.7557, Acc: 0.4582, F1: 0.2967 | Val Loss: 0.7516, Acc: 0.4635, F1: 0.2936 | Time: 83.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 09 | Train Loss: 0.7555, Acc: 0.4603, F1: 0.2977 | Val Loss: 0.7516, Acc: 0.4635, F1: 0.2936 | Time: 84.6s\n",
      "!!Training complete.!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "best_val_f1 = 0.0\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    t0 = time.time()\n",
    "\n",
    "    #train mode\n",
    "    model.train()\n",
    "    train_losses, y_true_train, y_pred_train = [], [], []\n",
    "\n",
    "    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\", leave=False)\n",
    "    for images, texts, targets in train_bar:\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # model expects tokenized text\n",
    "        tokenized = clip.tokenize(list(texts), truncate=True).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(images=images, tokenized_text=tokenized)\n",
    "        loss = criterion(logits, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        # prediction ( detach to avoid tracking in autograd )\n",
    "        preds = logits.argmax(dim=1).detach().cpu().numpy()\n",
    "        y_pred_train.extend(preds.tolist())\n",
    "        y_true_train.extend(targets.detach().cpu().numpy().tolist())\n",
    "\n",
    "        # update tqdm bar\n",
    "        train_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    train_loss = sum(train_losses) / len(train_losses)\n",
    "    train_acc = accuracy_score(y_true_train, y_pred_train)\n",
    "    train_f1 = f1_score(y_true_train, y_pred_train, average=\"weighted\")\n",
    "\n",
    "    # val mode\n",
    "    model.eval()\n",
    "    val_losses, y_true_val, y_pred_val = [], [], []\n",
    "\n",
    "    val_bar = tqdm(val_loader, desc=f\"Epoch {epoch}/{num_epochs} [Val]\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for images, texts, targets in val_bar:\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "            tokenized = clip.tokenize(list(texts), truncate=True).to(device)\n",
    "\n",
    "            logits = model(images=images, tokenized_text=tokenized)\n",
    "            loss = criterion(logits, targets)\n",
    "\n",
    "            val_losses.append(loss.item())\n",
    "            preds = logits.argmax(dim=1).cpu().numpy()\n",
    "            y_pred_val.extend(preds.tolist())\n",
    "            y_true_val.extend(targets.cpu().numpy().tolist())\n",
    "\n",
    "            # update tqdm bar\n",
    "            val_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    val_loss = sum(val_losses) / len(val_losses)\n",
    "    val_acc = accuracy_score(y_true_val, y_pred_val)\n",
    "    val_f1 = f1_score(y_true_val, y_pred_val, average=\"weighted\")\n",
    "\n",
    "    scheduler.step(val_f1)\n",
    "\n",
    "    print(f\"\\nEpoch {epoch:02d} | \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f} | \"\n",
    "          f\"Time: {(time.time()-t0):.1f}s\")\n",
    "\n",
    "    # saving best model\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        torch.save(model.state_dict(), \"best_multimodal_clip.pth\")\n",
    "        print(\"[NOTE]Saved best model.\")\n",
    "\n",
    "print(\"!!Training complete.!!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4c8897",
   "metadata": {},
   "source": [
    "The model doesn't seem to be learning well.  Many reasons may have been come to play.\n",
    "The model methodology is correct. we use pretrained CLIP encoders for image and text, project to a common latent space, and fuse them for classification. The observed low metrics can be attributed to frozen CLIP embeddings, limited dataset size, shallow classifier, and structured clinical text. Despite the metrics, the approach demonstrates a valid multimodal pipeline and can be justified for future work with larger datasets or fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71935c09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
